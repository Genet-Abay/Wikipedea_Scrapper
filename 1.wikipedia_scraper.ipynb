{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "from functools import lru_cache\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kt6JtKDpOAIe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VcbiLxfaVFOq"
   },
   "outputs": [],
   "source": [
    "cache = {}\n",
    "def hashable_cache(f):\n",
    "    def inner(url, session):\n",
    "        if url not in cache:\n",
    "            cache[url] = get_first_paragraph(url, session)\n",
    "        return cache[url]\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "T6FbVRw8Z3kA"
   },
   "outputs": [],
   "source": [
    "#@hashable_cache\n",
    "@lru_cache(maxsize = None)\n",
    "def get_first_paragraph(wikipedia_url, session_param):\n",
    "#     print(wikipedia_url)\n",
    "    req= session_param.get(wikipedia_url) # requests changed by session_param\n",
    "    content = req.text\n",
    "    soup = bs(content, 'html')\n",
    "    \n",
    "    #remove all the text link\n",
    "    for a in soup.findAll('a', href=True):\n",
    "        a.extract()\n",
    "        \n",
    "    paragraphs = soup.find_all('p')\n",
    "    first_paragraph_index = 0\n",
    "    i = 0\n",
    "    for paragraph in soup.find_all(\"p\"):   \n",
    "        if paragraph.find('b') != None:\n",
    "            first_paragraph_index = i            \n",
    "            break\n",
    "        i+=1\n",
    "    \n",
    "    first_paragraph = paragraphs[first_paragraph_index].text\n",
    "    \n",
    "    if wikipedia_url.startswith('https://en.'): #to reduce conflict with other language characters\n",
    "        first_paragraph = re.sub(r'[();{}[\\]]+', \"\", first_paragraph)\n",
    "        sanitized_paragraph = ' '.join(first_paragraph.strip().split())\n",
    "    else:\n",
    "        sanitized_paragraph = first_paragraph\n",
    "   \n",
    "    return sanitized_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_leaders():\n",
    "    global countries\n",
    "    \n",
    "    root_url = \"https://country-leaders.herokuapp.com\"\n",
    "    cookie_url = root_url + \"/cookie\"\n",
    "    country_url = root_url + \"/countries\"\n",
    "    leaders_url = root_url + \"/leaders\"\n",
    "\n",
    "    req_cookies = requests.get(cookie_url)\n",
    "    cookies=req_cookies.cookies\n",
    "   \n",
    "    req_countries = requests.get(country_url, cookies = cookies)\n",
    "    countries = req_countries.text\n",
    "#     print(countries)\n",
    "    \n",
    "    countries = countries.strip('[, ]')\n",
    "    countries = countries.split(\",\")    \n",
    "    \n",
    "    session = requests.Session()\n",
    "    leaders_per_country = {}\n",
    "    for country in countries:\n",
    "        country = country.replace('\\\"', \"\")\n",
    "        param = {'country': country}\n",
    "        \n",
    "        req_leaders = requests.get(leaders_url, cookies =cookies, params = param)\n",
    "        \n",
    "        if req_leaders.status_code == 403:\n",
    "            cookies=req_cookies.cookies\n",
    "            req_leaders = requests.get(leaders_url, cookies =cookies, params = param)\n",
    "            \n",
    "        content = req_leaders.text    \n",
    "        content = content.strip('[, ]')\n",
    "        list_leaders_currentcountry = content.split('}')\n",
    "#         print(\".......... \\nleaders info started here \\n\")\n",
    "        clean_leader_info_percountry = []\n",
    "        for leader_info in list_leaders_currentcountry:\n",
    "            leader_info_clean = leader_info.strip('{, }')\n",
    "            list_leader_info = leader_info_clean.split(',')\n",
    "            leader_fname = \"\"\n",
    "            leader_lname = \"\"\n",
    "            wiki_url = \"\"\n",
    "            \n",
    "            leader_info_dict={}\n",
    "            \n",
    "            for info in list_leader_info:                \n",
    "                if 'wikipedia' in info:\n",
    "                    wiki_split = info.split(':')\n",
    "                    wiki_url = (wiki_split[1] + \":\" + wiki_split[2]).replace(\"\\\"\", \"\")\n",
    "                elif \"first_name\" in info:\n",
    "                    leader_fname = info.split(':')[1].replace(\"\\\"\", \"\")\n",
    "                    \n",
    "                elif \"last_name\" in info:\n",
    "                    leader_lname = info.split(':')[1].replace(\"\\\"\", \"\")\n",
    "                    \n",
    "                \n",
    "                if leader_fname == \"\" or leader_lname == \"\" or wiki_url == \"\":\n",
    "                    continue\n",
    "                else:\n",
    "                    break #has to break out from this loop because no neet to travel to all information\n",
    "            \n",
    "            #break\n",
    "            try: \n",
    "                first_paragraph = get_first_paragraph(wiki_url, session)  \n",
    "            except:\n",
    "                first_paragraph = \"first paragraph could not be extracted. Either link not found or link has error\"\n",
    "                  \n",
    "            print(leader_fname + \" \" + leader_lname + \",  \" + wiki_url )\n",
    "            print(first_paragraph)\n",
    "            leader_info_dict['first_name'] = leader_fname\n",
    "            leader_info_dict['last_name'] = leader_lname\n",
    "            leader_info_dict['wikipedia_url'] = wiki_url\n",
    "            leader_info_dict['first_paragraph'] = first_paragraph\n",
    "            \n",
    "            clean_leader_info_percountry.append(leader_info_dict) #list of leaders info for current country under the loop\n",
    "        leaders_per_country[country] = clean_leader_info_percountry\n",
    "    return leaders_per_country     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zd9uYmq0aEQa"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# leaders_per_country = get_leaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtLTvw3wPqe0"
   },
   "source": [
    "## Saving the information created\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fW_U7gXktyv"
   },
   "source": [
    "Make a function `save()` to call this code easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(dir = \"C:/BeCode/LocalRepos/output_all_country/\"):\n",
    "    for country in countries:\n",
    "        try:\n",
    "            country = country.replace('\\\"', \"\")\n",
    "            file_name = dir + country + \"_leaders.json\"\n",
    "            json_file = open(file_name, 'w')\n",
    "            json_file.write(json.dumps(leaders_per_country.get(country)))\n",
    "            json_file.close()\n",
    "        except IOError:\n",
    "            print(\"cant write the file content in the country: \" + country)\n",
    "        else:\n",
    "            print(\"file successfully written\")\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "EfknpnTljqUd"
   },
   "outputs": [],
   "source": [
    "def read_leaders_info(country='us'):\n",
    "    try:\n",
    "        file_name = \"C:/BeCode/LocalRepos/Wikipedea_Scrapper/\" + country + \"_leaders.json\"\n",
    "        file_json = open(file_name, 'r')\n",
    "        data = json.load(file_json)\n",
    "        file_json.close()\n",
    "    except IOError:\n",
    "        print(\"problem with reading file, check if it exists\")\n",
    "    else:\n",
    "        return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lWQ6bbn31cix"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file successfully written\n",
      "file successfully written\n",
      "file successfully written\n",
      "file successfully written\n",
      "file successfully written\n"
     ]
    }
   ],
   "source": [
    "save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_leaders_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "wikiedia_scraper",
   "language": "python",
   "name": "wikiedia_scraper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
