{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Kt6JtKDpOAIe"
   },
   "outputs": [],
   "source": [
    "def get_leaders():\n",
    "    global countries\n",
    "    root_url = \"https://country-leaders.herokuapp.com\"\n",
    "    cookie_url = root_url + \"/cookie\"\n",
    "    country_url = root_url + \"/countries\"\n",
    "    leaders_url = root_url + \"/leaders\"\n",
    "\n",
    "    req_cookies = requests.get(cookie_url)\n",
    "    cookies=req_cookies.cookies\n",
    "   \n",
    "    req_countries = requests.get(country_url, cookies = cookies)\n",
    "    countries = req_countries.text\n",
    "#     print(countries)\n",
    "    \n",
    "    countries = countries.strip('[, ]')\n",
    "    countries = countries.split(\",\")    \n",
    "    \n",
    "    session = requests.Session()\n",
    "    leaders_per_country = {}\n",
    "    for country in countries:\n",
    "        country = country.replace('\\\"', \"\")\n",
    "        param = {'country': country}\n",
    "        \n",
    "        req_leaders = requests.get(leaders_url, cookies =cookies, params = param)\n",
    "        \n",
    "        if req_leaders.status_code == 403:\n",
    "            cookies=req_cookies.cookies\n",
    "            req_leaders = requests.get(leaders_url, cookies =cookies, params = param)\n",
    "            \n",
    "        content = req_leaders.text    \n",
    "        content = content.strip('[, ]')\n",
    "        list_leaders_currentcountry = content.split('}')\n",
    "#         print(\".......... \\nleaders info started here \\n\")\n",
    "        clean_leader_info_percountry = []\n",
    "        for leader_info in list_leaders_currentcountry:\n",
    "            leader_info_clean = leader_info.strip('{, }')\n",
    "            list_leader_info = leader_info_clean.split(',')\n",
    "            leader_fname = \"\"\n",
    "            leader_lname = \"\"\n",
    "            wiki_url = \"\"\n",
    "            \n",
    "            leader_info_dict={}\n",
    "            \n",
    "            for info in list_leader_info:                \n",
    "                if 'wikipedia' in info:\n",
    "                    wiki_split = info.split(':')\n",
    "                    wiki_url = (wiki_split[1] + \":\" + wiki_split[2]).replace(\"\\\"\", \"\")\n",
    "                elif \"first_name\" in info:\n",
    "                    leader_fname = info.split(':')[1].replace(\"\\\"\", \"\")\n",
    "                    \n",
    "                elif \"last_name\" in info:\n",
    "                    leader_lname = info.split(':')[1].replace(\"\\\"\", \"\")\n",
    "                    \n",
    "                \n",
    "                if leader_fname == \"\" or leader_lname == \"\" or wiki_url == \"\":\n",
    "                    continue\n",
    "                else:\n",
    "                    break #has to break out from this loop because no neet to travel to all information\n",
    "            print(leader_fname + \" \" + leader_lname + \",  \" + wiki_url)\n",
    "            #break\n",
    "            try: \n",
    "                first_paragraph = get_first_paragraph_withCashWrapper(wiki_url, session)  \n",
    "            except:\n",
    "                first_paragraph = \"Information could not be extracted. Either link not found or link has error\"\n",
    "                  \n",
    "            print(first_paragraph)\n",
    "            leader_info_dict['first_name'] = leader_fname\n",
    "            leader_info_dict['last_name'] = leader_lname\n",
    "            leader_info_dict['wikipedia_url'] = wiki_url\n",
    "            leader_info_dict['first_paragraph'] = first_paragraph\n",
    "            \n",
    "            clean_leader_info_percountry.append(leader_info_dict)\n",
    "        leaders_per_country[country] = clean_leader_info_percountry\n",
    "    return leaders_per_country        \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "VcbiLxfaVFOq"
   },
   "outputs": [],
   "source": [
    "cache = {}\n",
    "def hashable_cache(f):\n",
    "    def inner(url, session):\n",
    "        if url not in cache:\n",
    "            cache[url] = f(url, session)\n",
    "        return cache[url]\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "T6FbVRw8Z3kA"
   },
   "outputs": [],
   "source": [
    "#@hashable_cache\n",
    "@lru_cache(maxsize = 128)\n",
    "def get_first_paragraph_withCashWrapper(wikipedia_url, session_param):\n",
    "#     print(wikipedia_url)\n",
    "    req= session_param.get(wikipedia_url) # requests changed by session_param\n",
    "    content = req.text\n",
    "    soup = bs(content, 'html')\n",
    "    \n",
    "    #remove all the text link\n",
    "    for a in soup.findAll('a', href=True):\n",
    "        a.extract()\n",
    "        \n",
    "    paragraphs = soup.find_all('p')\n",
    "    first_paragraph_index = 0\n",
    "    i = 0\n",
    "    for paragraph in soup.find_all(\"p\"):   \n",
    "        if paragraph.find('b') != None:\n",
    "            first_paragraph_index = i            \n",
    "            break\n",
    "        i+=1\n",
    "    \n",
    "    first_paragraph = paragraphs[first_paragraph_index].text\n",
    "    \n",
    "    first_paragraph = re.sub(r'[();{}[\\]]+', \"\", first_paragraph)\n",
    "    sanitized_paragraph = ' '.join(first_paragraph.strip().split())\n",
    "    sanitized_paragraph = re.sub(r'II', 'II,',sanitized_paragraph)\n",
    "   \n",
    "    return sanitized_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ru.wikipedia.org/wiki/%D0%95%D0%BB%D1%8C%D1%86%D0%B8%D0%BD\n",
      "Бори́с Никола́евич Е́льцин 1931-02-01, , , , — , , — и , государственный и политический деятель, первый всенародно избранный в ноябре 1991 — июне 1992 года одновременно возглавлял . С марта по май 1992 года исполнял обязанности .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s= requests.Session()\n",
    "p = get_first_paragraph_withCashWrapper(\"https://ru.wikipedia.org/wiki/%D0%95%D0%BB%D1%8C%D1%86%D0%B8%D0%BD\", s)\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "id": "Zd9uYmq0aEQa"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# leaders_per_country = get_leaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtLTvw3wPqe0"
   },
   "source": [
    "## Saving the work\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "pTNGKKrOjNDk"
   },
   "outputs": [],
   "source": [
    "# 3 lines\n",
    "for country in countries:\n",
    "    country = country.replace('\\\"', \"\")\n",
    "    file_name = \"C:/BeCode/LocalRepos/GNT-Arai-4/\" + country + \"_\" + \"leaders.json\"\n",
    "    \n",
    "    json_file = open(file_name, 'w')\n",
    "    json_file.write(json.dumps(leaders_per_country.get(country)))\n",
    "    json_file.close()\n",
    "#     break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fW_U7gXktyv"
   },
   "source": [
    "Make a function `save()` to call this code easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EfknpnTljqUd"
   },
   "outputs": [],
   "source": [
    "\n",
    "# file_json = open(r\"C:\\BeCode\\LocalRepos\\GNT-Arai-4\\us_leaders.json\", 'r')\n",
    "# data = json.load(file_json)\n",
    "# print(data)\n",
    "    \n",
    "# file_json.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWQ6bbn31cix"
   },
   "outputs": [],
   "source": [
    "# Call the function\n",
    "# get_leaders()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "wikiedia_scraper",
   "language": "python",
   "name": "wikiedia_scraper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
